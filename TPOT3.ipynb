{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chethan\\Anaconda3\\lib\\site-packages\\deap\\tools\\_hypervolume\\pyhv.py:33: ImportWarning: Falling back to the python version of hypervolume module. Expect this to be very slow.\n",
      "  \"module. Expect this to be very slow.\", ImportWarning)\n",
      "C:\\Users\\chethan\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\chethan\\Anaconda3\\lib\\importlib\\_bootstrap_external.py:426: ImportWarning: Not importing directory C:\\Users\\chethan\\Anaconda3\\lib\\site-packages\\mpl_toolkits: missing __init__\n",
      "  _warnings.warn(msg.format(portions[0]), ImportWarning)\n",
      "C:\\Users\\chethan\\Anaconda3\\lib\\importlib\\_bootstrap_external.py:426: ImportWarning: Not importing directory c:\\users\\chethan\\anaconda3\\lib\\site-packages\\mpl_toolkits: missing __init__\n",
      "  _warnings.warn(msg.format(portions[0]), ImportWarning)\n",
      "C:\\Users\\chethan\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.feature_selection import RFE, f_regression\n",
    "from sklearn.linear_model import (LinearRegression, Ridge, Lasso, RandomizedLasso)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import sklearn \n",
    "from sklearn import preprocessing \n",
    "from tpot import TPOTRegressor   \n",
    "from sklearn.datasets import load_digits  \n",
    "from sklearn.cross_validation import train_test_split  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib\n",
    "from urllib import request\n",
    "import sklearn \n",
    "from statistics import mean \n",
    "import statistics\n",
    "from math import sqrt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from datetime import time\n",
    "from sklearn import feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_table(\"C:\\\\Users\\\\chethan\\\\Downloads\\\\historical_data1_Q12005.txt\",sep='\\|',names = ('fico', 'dt_first_pi', 'flag_fthb', 'dt_matr', 'cd_msa', 'mi_pct',\n",
    "       'cnt_units', 'occpy_sts', 'cltv', 'dti', 'orig_upb', 'ltv',\n",
    "       'int_rt', 'channel', 'ppmt_pnlty', 'prod_type', 'st', 'prop_type',\n",
    "       'zipcode', 'id_loan', 'loan_purpose', 'orig_loan_term', 'cnt_borr',\n",
    "       'seller_name', 'servicer_name', 'flag_sc', 'Year'),engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_table(\"C:\\\\Users\\\\chethan\\\\Downloads\\\\historical_data1_Q22005.txt\",sep='\\|',names = ('fico', 'dt_first_pi', 'flag_fthb', 'dt_matr', 'cd_msa', 'mi_pct',\n",
    "       'cnt_units', 'occpy_sts', 'cltv', 'dti', 'orig_upb', 'ltv',\n",
    "       'int_rt', 'channel', 'ppmt_pnlty', 'prod_type', 'st', 'prop_type',\n",
    "       'zipcode', 'id_loan', 'loan_purpose', 'orig_loan_term', 'cnt_borr',\n",
    "       'seller_name', 'servicer_name', 'flag_sc', 'Year'),engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######Data Cleaning##########\n",
    "def cleandata(df):\n",
    "\n",
    "\n",
    "    \n",
    "    #To convert the categorical to numerical\n",
    "    \n",
    "    df_sccstats = pd.get_dummies(df['occpy_sts'])\n",
    "    df_sccstats.columns = ['OS_I', 'OS_P', 'OS_S']\n",
    "    del df_sccstats['OS_I']\n",
    "    print(df_sccstats.head())\n",
    "    \n",
    "    df_channel = pd.get_dummies(df['channel'])\n",
    "    df_channel.columns = ['C_B', 'C_C', 'C_R','C_T']\n",
    "    del df_channel['C_B']\n",
    "    print(df_channel.head())\n",
    "    \n",
    "    df_ppmt = pd.get_dummies(df['ppmt_pnlty'])\n",
    "    df_ppmt.columns = ['ppmt_N0', 'ppmt_YES']\n",
    "    del df_ppmt['ppmt_YES']\n",
    "    print(df_ppmt.head())\n",
    "    \n",
    "    df_proptype = pd.get_dummies(df['prop_type'])\n",
    "    del df_proptype['99']\n",
    "    print(df_proptype.head())\n",
    "    \n",
    "    df_loanpurpose = pd.get_dummies(df['loan_purpose'])\n",
    "    print(df_loanpurpose.head())\n",
    "    \n",
    "    df_prodtype = pd.get_dummies(df['prod_type'])\n",
    "    print(df_prodtype.head())\n",
    "    \n",
    "    df_fthb = pd.get_dummies(df['flag_fthb'])\n",
    "    del df_fthb['9']\n",
    "    df_fthb.columns = ['fthb_N', 'fthb_Y']\n",
    "    print(df_fthb.head())\n",
    "    \n",
    "    df = df.join(df_sccstats)\n",
    "    df = df.join(df_channel)\n",
    "    df = df.join(df_ppmt)\n",
    "    df = df.join(df_prodtype)\n",
    "    df = df.join(df_proptype)\n",
    "    df = df.join(df_loanpurpose)\n",
    "    df = df.join(df_fthb)\n",
    "    del df['occpy_sts']\n",
    "    del df['channel']\n",
    "    del df['ppmt_pnlty']\n",
    "    del df['prod_type']\n",
    "    del df['prop_type']\n",
    "    del df['loan_purpose']\n",
    "    del df['flag_fthb']\n",
    "    \n",
    "    #To fill the missing values\n",
    "    df['cd_msa'].fillna(value=0,inplace= True)\n",
    "    df['mi_pct'].fillna(value=999,inplace= True)\n",
    "    df['cnt_units'].fillna(value=99,inplace= True)\n",
    "    df['cltv'].fillna(value=999,inplace= True)\n",
    "    df['dti'].fillna(value=999,inplace= True)\n",
    "    df['ltv'].fillna(value=999,inplace= True)\n",
    "    df.fillna(value=0,inplace= True)\n",
    "\n",
    "    #dropping these features\n",
    "    del df['id_loan']\n",
    "    del df['seller_name']\n",
    "    del df['servicer_name']\n",
    "    del df['st']\n",
    "    print(\"Final columns : \")\n",
    "    print(df.columns)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   OS_P  OS_S\n",
      "0     1     0\n",
      "1     1     0\n",
      "2     1     0\n",
      "3     0     1\n",
      "4     1     0\n",
      "   C_C  C_R  C_T\n",
      "0    0    1    0\n",
      "1    0    1    0\n",
      "2    0    1    0\n",
      "3    0    1    0\n",
      "4    0    1    0\n",
      "   ppmt_N0\n",
      "0        1\n",
      "1        1\n",
      "2        1\n",
      "3        1\n",
      "4        1\n",
      "   CO  CP  MH  PU  SF\n",
      "0   0   0   0   0   1\n",
      "1   0   0   0   0   1\n",
      "2   0   0   0   0   1\n",
      "3   1   0   0   0   0\n",
      "4   0   0   0   0   1\n",
      "   C  N  P\n",
      "0  1  0  0\n",
      "1  0  1  0\n",
      "2  0  0  1\n",
      "3  0  0  1\n",
      "4  1  0  0\n",
      "   FRM\n",
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "   fthb_N  fthb_Y\n",
      "0       1       0\n",
      "1       1       0\n",
      "2       1       0\n",
      "3       1       0\n",
      "4       1       0\n",
      "Final columns : \n",
      "Index(['fico', 'dt_first_pi', 'dt_matr', 'cd_msa', 'mi_pct', 'cnt_units',\n",
      "       'cltv', 'dti', 'orig_upb', 'ltv', 'int_rt', 'zipcode', 'orig_loan_term',\n",
      "       'cnt_borr', 'flag_sc', 'Year', 'OS_P', 'OS_S', 'C_C', 'C_R', 'C_T',\n",
      "       'ppmt_N0', 'FRM', 'CO', 'CP', 'MH', 'PU', 'SF', 'C', 'N', 'P', 'fthb_N',\n",
      "       'fthb_Y'],\n",
      "      dtype='object')\n",
      "   OS_P  OS_S\n",
      "0     1     0\n",
      "1     1     0\n",
      "2     1     0\n",
      "3     1     0\n",
      "4     1     0\n",
      "   C_C  C_R  C_T\n",
      "0    0    1    0\n",
      "1    0    1    0\n",
      "2    0    1    0\n",
      "3    0    1    0\n",
      "4    0    1    0\n",
      "   ppmt_N0\n",
      "0        1\n",
      "1        1\n",
      "2        1\n",
      "3        1\n",
      "4        1\n",
      "   CO  CP  MH  PU  SF\n",
      "0   0   0   0   0   1\n",
      "1   0   0   0   0   1\n",
      "2   0   0   0   1   0\n",
      "3   0   0   0   0   1\n",
      "4   0   0   0   0   1\n",
      "   C  N  P\n",
      "0  1  0  0\n",
      "1  0  1  0\n",
      "2  0  0  1\n",
      "3  0  1  0\n",
      "4  0  1  0\n",
      "   FRM\n",
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "   fthb_N  fthb_Y\n",
      "0       1       0\n",
      "1       1       0\n",
      "2       1       0\n",
      "3       1       0\n",
      "4       1       0\n",
      "Final columns : \n",
      "Index(['fico', 'dt_first_pi', 'dt_matr', 'cd_msa', 'mi_pct', 'cnt_units',\n",
      "       'cltv', 'dti', 'orig_upb', 'ltv', 'int_rt', 'zipcode', 'orig_loan_term',\n",
      "       'cnt_borr', 'flag_sc', 'Year', 'OS_P', 'OS_S', 'C_C', 'C_R', 'C_T',\n",
      "       'ppmt_N0', 'FRM', 'CO', 'CP', 'MH', 'PU', 'SF', 'C', 'N', 'P', 'fthb_N',\n",
      "       'fthb_Y'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Cleaned Final DataFrame\n",
    "df_train_cleaned = cleandata(df_train)\n",
    "df_test_cleaned = cleandata(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt = df_train_cleaned['int_rt']\n",
    "del df_train_cleaned['int_rt']\n",
    "ytest = df_test_cleaned['int_rt']\n",
    "del df_test_cleaned['int_rt']\n",
    "X_train = df_train_cleaned\n",
    "X_test = df_test_cleaned\n",
    "y_train = yt\n",
    "y_test = ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chethan\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: xgboost.XGBRegressor is not available and will not be used by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=400, style=ProgressStyle(descriptâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: -0.07463868521317207\n",
      "Generation 2 - Current best internal CV score: -0.07463868521317207\n",
      "Generation 3 - Current best internal CV score: -0.07463868521317207\n",
      "\n",
      "Best pipeline: RandomForestRegressor(input_matrix, bootstrap=True, max_features=0.3, min_samples_leaf=6, min_samples_split=4, n_estimators=100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TPOTRegressor(config_dict=None, crossover_rate=0.1, cv=5,\n",
       "       disable_update_check=False, early_stop=None, generations=3,\n",
       "       max_eval_time_mins=5, max_time_mins=None, memory=None,\n",
       "       mutation_rate=0.9, n_jobs=1, offspring_size=None,\n",
       "       periodic_checkpoint_folder=None, population_size=100,\n",
       "       random_state=None, scoring=None, subsample=1.0, use_dask=False,\n",
       "       verbosity=2, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot = TPOTRegressor(generations=3, verbosity=2)  \n",
    "tpot.fit(X_train, y_train)  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.08672433863806606\n"
     ]
    }
   ],
   "source": [
    "print(tpot.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypredtpot_test = tpot.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypredtpot_train = tpot.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(org,pred):\n",
    "    return mean_absolute_error(org,pred)\n",
    "def rmse(org,pred):\n",
    "    return np.sqrt(mean_squared_error(org,pred))\n",
    "def mape(org,pred):\n",
    "    y_true, y_pred = np.array(org), np.array(pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(model,yorg,ypred):\n",
    "    print(\"The scores for the model : \",model)\n",
    "    print(\"MAE    :    \",mae(yorg,ypred))\n",
    "    print(\"RMSE   :    \",rmse(yorg,ypred))\n",
    "    print(\"MAPE   :    \",mape(yorg,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for test\n",
      "The scores for the model :  TPOT\n",
      "MAE    :     0.22545943023682835\n",
      "RMSE   :     0.294489963560842\n",
      "MAPE   :     3.9022006060336705\n"
     ]
    }
   ],
   "source": [
    "print(\"Scores for test\")\n",
    "scores(\"TPOT\",y_test,ypredtpot_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for train\n",
      "The scores for the model :  TPOT\n",
      "MAE    :     0.15370824295682756\n",
      "RMSE   :     0.21097243449412878\n",
      "MAPE   :     2.718942174159185\n"
     ]
    }
   ],
   "source": [
    "print(\"Scores for train\")\n",
    "scores(\"TPOT\",y_train,ypredtpot_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of TPOTRegressor(config_dict=None, crossover_rate=0.1, cv=5,\n",
       "       disable_update_check=False, early_stop=None, generations=3,\n",
       "       max_eval_time_mins=5, max_time_mins=None, memory=None,\n",
       "       mutation_rate=0.9, n_jobs=1, offspring_size=None,\n",
       "       periodic_checkpoint_folder=None, population_size=100,\n",
       "       random_state=None, scoring=None, subsample=1.0, use_dask=False,\n",
       "       verbosity=2, warm_start=False)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features=0.3, max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=6, min_samples_split=4,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot.fitted_pipeline_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot.export('tpot_exported_pipeline.py')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
